---
title: "Regression Analysis"
date: "`r Sys.Date()`"
runtime: html
output: 
  html_document:
    fig_width: 9
    fig_height: 6
    fig_caption: true
    number_sections: true
    toc: false
    toc_float:
      collapsed: false
---

```{r setup, include=FALSE}
rm(list=ls())

knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, dev = 'svg')

# For testing
devtools::load_all("..")

# For production
# library(EcologyCore)

library(dplyr)
```

```{r params, include=FALSE}
regression_method <- "forward" #exhaustive, backward, forward, seqrep
really.big <- FALSE #TRUE/FALSE
num_top_models <- 5
validation_method <- "cv"
k_fold <- 5
```

```{r data_import, include=FALSE, echo=TRUE}
meta_path <- "/Users/reuben/Library/CloudStorage/OneDrive-SharedLibraries-CranfieldUniversity/Sophie Bretagne - Shared/data/metadata.csv"
alpha_path <- "/Users/reuben/Library/CloudStorage/OneDrive-SharedLibraries-CranfieldUniversity/Sophie Bretagne - Shared/results/alpha_diversity/AlphaDiversity.csv"
enviro_path <- "/Users/reuben/Library/CloudStorage/OneDrive-SharedLibraries-CranfieldUniversity/Sophie Bretagne - Shared/results/environmental_filtering/EnvironmentalFiltering.csv"

meta_table <- read.csv(meta_path, header = TRUE, row.names = 1)
alpha_table <- read.csv(alpha_path, header = TRUE, row.names = 1)
enviro_table <- read.csv(enviro_path, header = TRUE, row.names = 1)

# Find common row names in all three tables
common_rows <- Reduce(intersect, list(rownames(meta_table), rownames(alpha_table), rownames(enviro_table)))

# Subset each table to keep only the common rows
meta_table_filtered <- meta_table[common_rows, , drop = FALSE]
alpha_table_filtered <- alpha_table[common_rows, , drop = FALSE]
enviro_table_filtered <- enviro_table[common_rows, , drop = FALSE]

# Combine tables by row names
df <- cbind(meta_table_filtered, alpha_table_filtered, enviro_table_filtered)
```

```{r hypothesis, include=FALSE}
response_variables <- c("Shannon", "Richness", "NRI", "NTI")
explanatory_variables <- c(
  "pH",
  "Headloss",
  "temperature..C.",
  "Neat.coliform.2000",
  "neat.e.coli.2000",
  "Cl.perfrigens.conf",
  "Enterococci.confirmed.100ml",
  "X22C.Plate.Count.Neat",
  "Conductivity.20C",
  "Turbidity.FTU",
  "Ammoniacal.Nitrogen",
  "Ammonium.NH4",
  "Nitrate.as.N",
  "Nitrite.as.N",
  "Alkalinity.as.CaCO3",
  "Sulphate.as.SO4",
  "Carbon.Total.Organic"
)
```

# Regression Analysis

We use a best subsets approach to correlate environmental variables with microbiome response metrics.

## Correlation Matrix

The following matrix shows the R^2 correlation between each pair of explanatory variables. Note that values > 0.9 indicate a linear dependence between predictors which is likely to artificially inflate beta coefficients. In this case, it is better to remove one of the predictors, or transform the data in some way to mask the effect.

```{r corr_matrix, echo=FALSE}
corr_matrix <- cor(df[,explanatory_variables], use = "pairwise.complete.obs")

ggcorrplot::ggcorrplot(corr_matrix, type = "upper", lab = TRUE, colors = c("red", "white", "blue")) + 
  ggtitle("Correlation Matrix of Predictors")
```

The function regsubsets() from the leaps R package (Lumley and Miller, 2009) is used to systematically build regression models to explain each metric via subsets of explanatory variables. Each model is then evaluated by using the train() function from the caret R package (Kuhn, 2008) to compute the k-fold cross-validation error (with k = 5). Statistics for each model are obtained using the tab_model() function from the sjPlot package (LÃ¼decke, 2018).

## Best Models

This table shows the best model for each response variable.

```{r analyse, include=FALSE}
results <- lapply(response_variables, function(x) diversity_regression(df, x, explanatory_variables, regression_method, really.big))
results <- setNames(results, response_variables)
```

```{r, results='asis', echo=FALSE}
#cv_errors <- lapply(1:length(results), function(i) {
for (i in 1:length(results)) {
  cat(sprintf("### %s\n\n", response_variables[i]))

  # Ensure that we are only selecting samples for which the meta_table[,explanatory_variables] is complete
  # TODO: add interpolation options?
  lm.dat <- df
  
  lm.dat[, explanatory_variables] <- lapply(lm.dat[,explanatory_variables], function(x) as.numeric(as.character(x)))
  lm.dat <- lm.dat[complete.cases(lm.dat[, explanatory_variables]),]
  lm.dat <- lm.dat[complete.cases(lm.dat[, response_variables[i]]), , drop = FALSE]

  lm.dat <- data.frame(lm.dat[, response_variables[i], drop = FALSE], lm.dat[, explanatory_variables, drop = FALSE])
  
  cv <- cross_validate(results[[i]], response_variables[i], lm.dat)
  
  cat(sprintf("\n\nThe following table shows the top %d models, ranked by k-fold cross-validation error. Lowest error is best.\n\n", num_top_models))

  print(kableExtra::kbl(cv, format = "html", digits = 5, caption = sprintf("Cross-validation Errors for %s Models", response_variables[i])) %>%
    kableExtra::kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = FALSE))
  
  cat(sprintf("\n\nThis heatmap shows the coefficients of the top %d models, ranked by k-fold cross-validation error.\n\n", num_top_models))
  
  model_details <- get_model_details(results[[i]], response_variables[i], lm.dat)
  print(heatmap_all_models(model_details))

  cat(sprintf("\n\nThe following tables show summary statistics for the top %d models, ranked by k-fold cross-validation error.\n\n", num_top_models))
  
  ms <- get_model_summaries(model_details)
  
  for (model in names(ms)) {
    print(kableExtra::kbl(ms[[model]], format = "html", digits = 5, escape = TRUE, caption = sprintf("Summary Statistics for %s", model)) %>%
      kableExtra::kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = FALSE))
  }
}

  #return(cv)
  #cat("This is the best model as determined by k-fold cross-validation error.\n\n")
  #best <- best_model(results[[i]], response_variables[i], lm.dat)
  #print(summary(best))
#})
```

## Summary Table

```{r combined_heatmap, echo=FALSE}
# Create a list to store model details for each response variable
all_response_model_details <- list()
# Also store the order information for each response variable
model_orders <- list()

# Loop through each response variable to collect model details
for (i in 1:length(response_variables)) {
  # Ensure that we are only selecting samples for which the meta_table[,explanatory_variables] is complete
  lm.dat <- df
  
  lm.dat[, explanatory_variables] <- lapply(lm.dat[,explanatory_variables], function(x) as.numeric(as.character(x)))
  lm.dat <- lm.dat[complete.cases(lm.dat[, explanatory_variables]),]
  lm.dat <- lm.dat[complete.cases(lm.dat[, response_variables[i]]), , drop = FALSE]

  lm.dat <- data.frame(lm.dat[, response_variables[i], drop = FALSE], lm.dat[, explanatory_variables, drop = FALSE])
  
  # Get cross-validation errors to determine model order
  cv <- cross_validate(results[[i]], response_variables[i], lm.dat)
  
  # Get model details for this response variable
  model_details <- get_model_details(results[[i]], response_variables[i], lm.dat)
  
  # Store in the list with the response variable name
  all_response_model_details[[response_variables[i]]] <- model_details
  
  # Store the order of models based on CV error
  model_orders[[response_variables[i]]] <- rownames(cv)
}

# Function to create a combined heatmap of all models across all response variables
combined_heatmap <- function(all_response_model_details, model_orders) {
  # Create a combined dataframe of all models' coefficients and significance
  all_models_coef <- do.call(rbind, lapply(names(all_response_model_details), function(response_var) {
    model_details <- all_response_model_details[[response_var]]
    model_names <- names(model_details)
    
    # Get the order for this response variable
    order <- model_orders[[response_var]]
    
    combined_table <- do.call(rbind, lapply(seq_along(model_details), function(i) {
      x <- model_details[[i]]
      
      # Get model frame and clean variable names
      mf <- model.frame(x)
      var_names <- colnames(mf)[-1] # Exclude response variable
      
      # Get standardized coefficients
      standardized_model <- lm(scale(model.frame(x)[,1]) ~ scale(model.frame(x)[,-1]))
      std_coef <- coef(standardized_model)
      
      names(std_coef) <- c("(Intercept)", var_names)
      
      # Get p-values
      p_values <- summary(x)$coefficients[,4]
      
      # Create significance stars
      stars <- ifelse(p_values < 0.001, "***",
                      ifelse(p_values < 0.01, "**",
                             ifelse(p_values < 0.05, "*", "")))
      
      # Create a unique model name that includes rank information
      rank_in_order <- which(order == as.numeric(gsub("Model ", "", model_names[i])))
      model_display_name <- paste0("Rank ", rank_in_order)
      
      # Include all explanatory variables
      all_vars <- data.frame(
        Model = model_display_name,  # Use rank-based name
        OriginalModel = model_names[i],  # Keep original model name for reference
        ModelRank = rank_in_order,  # Store rank for ordering
        Term = c(var_names, setdiff(explanatory_variables, var_names)),  # Include all explanatory variables
        Estimate = c(std_coef[-1], rep(NA, length(setdiff(explanatory_variables, var_names)))),  # NA for non-included vars
        Stars = c(stars[-1], rep("", length(setdiff(explanatory_variables, var_names)))),  # No stars for non-included vars
        Response = response_var,
        stringsAsFactors = FALSE
      )
      
      return(all_vars)
    }))

    combined_table$Response <- factor(combined_table$Response, levels = response_variables)

    return(combined_table)
  }))
  
  # Remove intercept terms
  all_models_coef <- all_models_coef[all_models_coef$Term != "(Intercept)", ]
  
  # Create the combined heatmap
  p <- ggplot(all_models_coef, aes(x = ModelRank, y = Term, fill = Estimate)) +
    geom_tile() +
    geom_text(aes(label = Stars), color = "black", size = 3) +
    scale_fill_gradient2(low = "#2166AC", mid = "white", high = "#B2182B", midpoint = 0, name = "Coefficient", na.value = "white") +
    scale_x_continuous(breaks = 1:5, labels = paste0("Rank ", 1:5)) +
    theme_light() +
    facet_wrap(~ Response, nrow = 1, scales = "free_x") +
    theme(
      axis.text.x = element_text(angle = 45, size = 8),
      axis.text.y = element_text(size = 8),
      panel.grid.major = element_blank(),
      panel.grid.minor = element_blank(),
      strip.background = element_rect(fill = "#cdcdcd"),
      strip.text = element_text(color = "black")
    ) +
    labs(x = "Model Rank (by CV Error)", y = "Variable",
         title = "Coefficients Across Top Models for All Response Variables",
         subtitle = "* p<0.05, ** p<0.01, *** p<0.001")
  
  return(p)
}

# Generate and display the combined heatmap
cat("This heatmap shows the coefficients of the top 5 models for each response variable, faceted by response variable.\n\n")
print(combined_heatmap(all_response_model_details, model_orders))
```
